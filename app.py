from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
import time
import csv
import os

# CSV path (you can change this to an absolute path if needed)
csv_file = "riyadh_barbershop_template.csv"

# List of Riyadh districts
riyadh_districts = [
    "Ø§Ù„ØµØ­Ø§ÙØ©", "Ø§Ù„Ù†Ø±Ø¬Ø³", "Ø§Ù„ÙŠØ§Ø³Ù…ÙŠÙ†", "Ø§Ù„Ø¹Ø§Ø±Ø¶", "Ø§Ù„Ù‚ÙŠØ±ÙˆØ§Ù†", "Ø§Ù„Ø±Ø¨ÙŠØ¹", "Ø§Ù„Ù†Ø¯Ù‰", "Ø§Ù„ØºØ¯ÙŠØ±", "Ø§Ù„Ù…Ù„Ù‚Ø§",
    "Ø§Ù„Ù…Ø±Ø³Ù„Ø§Øª", "Ø§Ù„Ù…Ø±ÙˆØ¬", "Ø§Ù„Ø§Ø²Ø¯Ù‡Ø§Ø±", "Ø§Ù„ÙÙ„Ø§Ø­", "Ø§Ù„Ø¹Ù‚ÙŠÙ‚", "Ø§Ù„Ø³Ù…Ø­Ø§Ù†", "Ø­Ø·ÙŠÙ†", "Ø§Ù„Ù†Ø®ÙŠÙ„", "Ø§Ù„Ø±Ø¨ÙˆØ©",
    "Ø§Ù„ÙˆØ±ÙˆØ¯", "Ø§Ù„Ø±Ø­Ù…Ø§Ù†ÙŠØ©", "Ø§Ù„Ø±Ø§Ø¦Ø¯", "Ø§Ù„ÙˆØ§Ø¯ÙŠ"
]

# Setup Chrome options
options = webdriver.ChromeOptions()
options.add_argument("--start-maximized")
options.add_experimental_option("detach", True)

# Start Chrome driver
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# Write header only once
if not os.path.exists(csv_file):
    with open(csv_file, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(["Ø§Ø³Ù… Ø§Ù„ØµØ§Ù„ÙˆÙ†", "Ø§Ù„Ø­ÙŠ", "Ø§Ù„Ø¹Ù†ÙˆØ§Ù†", "Ø±Ù‚Ù… Ø§Ù„Ø¬ÙˆØ§Ù„", "Ø±Ø§Ø¨Ø· Ø§Ù„Ø¥Ù†Ø³ØªØºØ±Ø§Ù…", "Ø±Ø§Ø¨Ø· Ø§Ù„ÙˆØ§ØªØ³Ø§Ø¨", "Ù…Ù„Ø§Ø­Ø¸Ø§Øª"])

# Start scraping
for district in riyadh_districts:
    query = f"ØµØ§Ù„ÙˆÙ† Ø­Ù„Ø§Ù‚Ø© Ø±Ø¬Ø§Ù„ {district} Ø§Ù„Ø±ÙŠØ§Ø¶"
    print(f"ğŸ” Searching: {query}")
    driver.get("https://www.google.com/maps")
    time.sleep(3)

    try:
        # Search the query
        search_box = driver.find_element(By.ID, "searchboxinput")
        search_box.clear()
        search_box.send_keys(query)
        driver.find_element(By.ID, "searchbox-searchbutton").click()
        time.sleep(6)

        # Get list of barber shops in results
        results = driver.find_elements(By.CSS_SELECTOR, 'a.hfpxzc')[:5]

        print(f"ğŸ“ Found {len(results)} listings in {district}")

        for i, result in enumerate(results, 1):
            try:
                result.click()
                time.sleep(4)

                # Extract data from the side panel
                name = driver.find_element(By.CSS_SELECTOR, 'h1.DUwDvf').text
                try:
                    address = driver.find_element(By.CSS_SELECTOR, 'div[data-item-id="address"]').text
                except:
                    address = "ØºÙŠØ± Ù…ØªÙˆÙØ±"

                try:
                    phone = driver.find_element(By.CSS_SELECTOR, 'div[data-tooltip="Ù†Ø³Ø® Ø±Ù‚Ù… Ø§Ù„Ù‡Ø§ØªÙ"]').text
                except:
                    phone = "ØºÙŠØ± Ù…ØªÙˆÙØ±"

                # Instagram and WhatsApp: requires deep scraping (not easily available)
                insta = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                whatsapp = "ØºÙŠØ± Ù…ØªÙˆÙØ±"

                with open(csv_file, "a", newline="", encoding="utf-8-sig") as f:
                    writer = csv.writer(f)
                    writer.writerow([name, district, address, phone, insta, whatsapp, ""])
                print(f"âœ… Saved: {name}, {address}, {phone}")

            except Exception as e:
                print(f"âŒ Error in result {i} for {district}: {str(e)}")

    except Exception as e:
        print(f"âŒ Search error for {district}: {str(e)}")

# Done
driver.quit()
print(f"\nğŸ‰ All done. Data saved to: {csv_file}")
